{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6092787,"sourceType":"datasetVersion","datasetId":3489360}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Supervised Learning** (SL) is a ML Algorithm that is most commonly used in real world applications. Here, we will be focusing on a **Regression** Model, which predicts a number from infinitely many possible numbers. Several performance and optimization algorithms will be applied as we continue through the course to provide additional context on how these algorithms work and how they can be applied in the future. ","metadata":{}},{"cell_type":"markdown","source":"The dataset we're looking at contains the housing price predictions based on attributes such as the total area (squared feet), the number of bedrooms, and more. \n\nGoal: Use this dataset to create a linear regression model to predict the cost of a house based on the total area.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # To visualize\nimport seaborn as sns #To Visualize\nimport math\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-02-26T00:59:27.973504Z","iopub.execute_input":"2024-02-26T00:59:27.973825Z","iopub.status.idle":"2024-02-26T00:59:27.978177Z","shell.execute_reply.started":"2024-02-26T00:59:27.973789Z","shell.execute_reply":"2024-02-26T00:59:27.977367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/housing-price-prediction/Housing.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2024-02-25T23:33:49.830318Z","iopub.execute_input":"2024-02-25T23:33:49.830874Z","iopub.status.idle":"2024-02-25T23:33:49.882921Z","shell.execute_reply.started":"2024-02-25T23:33:49.830837Z","shell.execute_reply":"2024-02-25T23:33:49.881834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For simplicity, we'll begin with Univariate Linear Regression. We'll start by predicting the price of a house based on the area. By plotting the relationship between the Area (Input) and Price (Output), we can recognize the linearity occurring between the two, allowing for good use of a linear regression model. ","metadata":{}},{"cell_type":"code","source":"x = np.array(df['area']).reshape(-1, 1)\ny = np.array(df['price']).reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T23:33:51.392213Z","iopub.execute_input":"2024-02-25T23:33:51.392566Z","iopub.status.idle":"2024-02-25T23:33:51.399522Z","shell.execute_reply.started":"2024-02-25T23:33:51.392537Z","shell.execute_reply":"2024-02-25T23:33:51.398302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_regressor = LinearRegression()\nlinear_regressor.fit(x, y)\ny_pred = linear_regressor.predict(x)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T23:33:57.177893Z","iopub.execute_input":"2024-02-25T23:33:57.178235Z","iopub.status.idle":"2024-02-25T23:33:57.195367Z","shell.execute_reply.started":"2024-02-25T23:33:57.178206Z","shell.execute_reply":"2024-02-25T23:33:57.194149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **Cost Function** for univariate linear regression is: \n $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$\n \nIt's important to be able to properly visualize the Cost Function and be able to execute the method. Below are two options to help your understanding. The first allows you to adjust w and b as you see fit to best match the graph on the right to the trend line in the left. While the second, allows you to code the cost function out to better apply yourself to the concept. \n\nGoal: Minimize the cost of our model to better fit it to the data.  \n\nFor a deeper discussion visit [Medium](https://medium.com/@lachlanmiller_52885/understanding-and-calculating-the-cost-function-for-linear-regression-39b8a3519fcb). ","metadata":{}},{"cell_type":"markdown","source":"### **Implementing Cost Function**","metadata":{}},{"cell_type":"code","source":"figure, axis = plt.subplots(ncols=2, figsize=(15, 8))\n\nsns.scatterplot(x=\"area\", y=\"price\", data=df, ax=axis[0])\naxis[0].plot(np.unique(x.flatten()), np.poly1d(np.polyfit(x.flatten(), y.flatten(), 1))(np.unique(x)), c='g')\n\nsns.scatterplot(x=\"area\", y=\"price\", data=df, ax=axis[1])\n#Update the w and b values to help visualize where the line of best fit lies. \nw = 1\nb = 0\ny_fit = w * x + b\naxis[1].plot(x, y_fit, c='purple')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T00:53:55.127095Z","iopub.execute_input":"2024-02-26T00:53:55.127406Z","iopub.status.idle":"2024-02-26T00:53:55.538612Z","shell.execute_reply.started":"2024-02-26T00:53:55.127382Z","shell.execute_reply":"2024-02-26T00:53:55.537578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_cost(x, y, w, b): \n    #Add Code Here to Compute Cost of Model. ","metadata":{"execution":{"iopub.status.busy":"2024-02-26T15:11:55.048454Z","iopub.execute_input":"2024-02-26T15:11:55.049242Z","iopub.status.idle":"2024-02-26T15:11:55.085077Z","shell.execute_reply.started":"2024-02-26T15:11:55.049200Z","shell.execute_reply":"2024-02-26T15:11:55.083552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cost = compute_cost(x, y, w, b)\ncost","metadata":{"execution":{"iopub.status.busy":"2024-02-26T00:54:00.210458Z","iopub.execute_input":"2024-02-26T00:54:00.210780Z","iopub.status.idle":"2024-02-26T00:54:00.221101Z","shell.execute_reply.started":"2024-02-26T00:54:00.210755Z","shell.execute_reply":"2024-02-26T00:54:00.219878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Gradient Descent** is an optimization algorithm used to find a local minimum/maximum of the cost function *J(w,b)* and is described as:\n\n$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\newline\n\\; \\lbrace  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{2}, \n  \\; b = b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\rbrace \\newline \n\\end{align*}$$\nwhere, parameters $w$, $b$ are updated simultaneously.  \nThe gradient is defined as:\n$$\n\\begin{align}\n\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{3}\\\\\n  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{4}\\\\\n\\end{align}\n$$\n\nHere *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters.\n\nGoal: Optimize the Cost Function calculated above. \n\nVisit [Towards Data Science](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21) for more information. ","metadata":{}},{"cell_type":"markdown","source":"### **Implementing Gradient Descent** ","metadata":{}},{"cell_type":"code","source":"def compute_gradient(x, y, w, b): ","metadata":{"execution":{"iopub.status.busy":"2024-02-26T00:28:43.488034Z","iopub.execute_input":"2024-02-26T00:28:43.488363Z","iopub.status.idle":"2024-02-26T00:28:43.495170Z","shell.execute_reply.started":"2024-02-26T00:28:43.488338Z","shell.execute_reply":"2024-02-26T00:28:43.494043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gradient = compute_gradient(x, y, w, b)\ngradient","metadata":{"execution":{"iopub.status.busy":"2024-02-26T00:54:05.372197Z","iopub.execute_input":"2024-02-26T00:54:05.372551Z","iopub.status.idle":"2024-02-26T00:54:05.382711Z","shell.execute_reply.started":"2024-02-26T00:54:05.372525Z","shell.execute_reply":"2024-02-26T00:54:05.382019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_descent(x, y, w_in, b_in, alpha, num_iters): ","metadata":{"execution":{"iopub.status.busy":"2024-02-26T01:04:07.320448Z","iopub.execute_input":"2024-02-26T01:04:07.320784Z","iopub.status.idle":"2024-02-26T01:04:07.328018Z","shell.execute_reply.started":"2024-02-26T01:04:07.320761Z","shell.execute_reply":"2024-02-26T01:04:07.326455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gradient_descent_result = gradient_descent(x, y, 1, 0, 0.2, 24)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T01:04:12.473245Z","iopub.execute_input":"2024-02-26T01:04:12.473723Z","iopub.status.idle":"2024-02-26T01:04:12.618440Z","shell.execute_reply.started":"2024-02-26T01:04:12.473696Z","shell.execute_reply":"2024-02-26T01:04:12.617605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the learning rate is too small or too large, then the speed of gradient descent may be affected. Part of our job is to consider what methods could be used to help us realize these issues. In the cell below, find a way to best plot the gradient descent and the effects of a larger/smaller learning rate. ","metadata":{}},{"cell_type":"code","source":"#Place Two Plots Here. One when Learning Rate is small and another when Learning Rate is large. ","metadata":{"execution":{"iopub.status.busy":"2024-02-26T00:48:31.587265Z","iopub.execute_input":"2024-02-26T00:48:31.587597Z","iopub.status.idle":"2024-02-26T00:48:31.592571Z","shell.execute_reply.started":"2024-02-26T00:48:31.587572Z","shell.execute_reply":"2024-02-26T00:48:31.591167Z"},"trusted":true},"execution_count":null,"outputs":[]}]}